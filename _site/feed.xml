<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nikhil Anand</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 05 Nov 2024 19:33:07 +0530</pubDate>
    <lastBuildDate>Tue, 05 Nov 2024 19:33:07 +0530</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
      <item>
        <title>What makes RAG so effective? (How Attention Shapes Understanding in LLMs)</title>
        <description>&lt;div style=&quot;border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 17px; text-align: center&quot;&gt;
    &lt;i&gt;
        &lt;a href=&quot;https://medium.com/@nikhilanandnj/&quot; target=&quot;_blank&quot;&gt;READ THE FULL ARTICLE ON MEDIUM&lt;/a&gt;
    &lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Since ChatGPT came out, I remember seeing several blogs and videos talking about this new technique called Retrieval-Augmented Generation. It worked exceedingly well in preventing hallucinations, and ensuring factually correct outputs. Since then, startups and libraries have been built just to help people optimise this process for their use cases.&lt;/p&gt;

&lt;p&gt;What is Retrieval-Augmented Generation? RAG is a technique that uses an ability of LLMs known as &lt;strong&gt;In-Context Learning&lt;/strong&gt;. That means that if we provide a context with some information that an LLM doesn’t know, LLMs are adept at learning new information from the context and answering the question accordingly.&lt;/p&gt;

&lt;p&gt;For example, the user asks the LLM &lt;i&gt;“What is the most popular language spoken in Chennai?”&lt;/i&gt; and the LLM incorrectly outputs &lt;i&gt;“Hindi”&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../assets/images/chennai_hindi.png&quot; alt=&quot;drawing&quot; width=&quot;630&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In a RAG setup, the LLM might retrieve a Wikipedia article saying the following:&lt;/p&gt;

&lt;div style=&quot;border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 20px&quot;&gt;
&lt;i&gt;
&lt;b&gt;Context: &lt;/b&gt;
Tamil is the language spoken by most of Chennai's population; English is largely spoken by white-collar workers.[145][146] As per the 2011 census, Tamil is the most spoken language with 3,640,389 (78.3%) of speakers...
&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Once the LLM has retrieved this from the web, it prepends this to the question and feeds it back into the LLM to get an answer.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../assets/images/chennai_tamil.png&quot; alt=&quot;drawing&quot; width=&quot;630&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Providing the context leads to the correct answer, &lt;i&gt;“Tamil”&lt;/i&gt;. One of the advantages of RAG is that it helps reduce hallucinations by ensuring the LLM first retrieves facts from the web and then at least gets the facts right.&lt;/p&gt;

&lt;p&gt;But what’s going on internally?&lt;/p&gt;

&lt;h4 id=&quot;causal-tracing&quot;&gt;Causal Tracing&lt;/h4&gt;

&lt;p&gt;I wrote an &lt;a href=&quot;https://medium.com/@nikhilanandnj/where-are-facts-stored-in-large-language-models-0869914cfcbf&quot;&gt;earlier blog&lt;/a&gt; that explains the process of Causal Tracing in detail. I’d strongly recommend going through that (specifically the section on “Causal Mediation Analysis”) before reading this blog.&lt;/p&gt;

&lt;p&gt;Let’s consider the input: “What is the most popular language in &lt;u&gt;Chennai&lt;/u&gt;?”&lt;/p&gt;

&lt;p&gt;The idea behind causal tracing is figuring out which parts of the LLM are responsible for a particular behaviour. We saw over there that facts are stored in MLPs, and the MLP at the &lt;strong&gt;Last Subject Token (LST)&lt;/strong&gt; significantly impacts the output. In this case, “&lt;u&gt;Chennai&lt;/u&gt;” is the &lt;strong&gt;LST&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the clean case, we don’t perform corruption and the LLM outputs “Hindi”.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/mlp_causal1.png&quot; alt=&quot;drawing&quot; width=&quot;500&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If we corrupt the LLM, we get the wrong output, but after restoring only the MLP at the LST, we see that the LLM brings back the original output.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/mlp_causal2.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Restoring the MLP at the LST causes the probability of “Hindi” to rise, and we average this delta in probability over several statements, so we know it rises for multiple statements, not just one.&lt;/p&gt;

&lt;p&gt;On the other hand, let’s consider the RAG setup, which accurately outputs “Tamil” in the clean case.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/mlp_notcausal1.png&quot; alt=&quot;drawing&quot; width=&quot;620&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/mlp_notcausal2.png&quot; alt=&quot;drawing&quot; width=&quot;1000&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The hypothesis stated in our &lt;a href=&quot;https://medium.com/@nikhilanandnj/where-are-facts-stored-in-large-language-models-0869914cfcbf&quot;&gt;previous blog&lt;/a&gt; is that &lt;strong&gt;MLPs have something to do with factual recall&lt;/strong&gt;. However, restoring the MLP hidden states at the LST no longer returns the original output (“Tamil”). It looks like &lt;strong&gt;MLP activations aren’t that important anymore&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;That could mean the LLM is &lt;strong&gt;no longer utilising its parametric memory&lt;/strong&gt;. This makes sense since we expect it to focus on the context rather than use its parametric memory.&lt;/p&gt;

&lt;div style=&quot;border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 20px&quot;&gt;
&lt;i&gt;
&lt;b&gt;Finding #1:&lt;/b&gt; In the presence of a context, LLMs minimally use the parametric memory to answer a question.&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;So, if MLPs aren’t that important anymore, what is?&lt;/p&gt;

&lt;h4 id=&quot;how-attention-works-in-llms&quot;&gt;How Attention Works in LLMs&lt;/h4&gt;

&lt;p&gt;I’d recommend going over &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;Jay Allamar’s blog post&lt;/a&gt; “The Illustrated Transformer” to better understand how attention works.&lt;/p&gt;

&lt;p&gt;Attention is essential in LLMs because it allows LLMs to model the meaning of words based on other words in the input. For example, an “it” later in the sentence might refer to a noun that came before, and information needs to flow from the noun to the “it” to generate the next token. Thus, some dependencies must be facilitated across tokens to get the value of the attention block’s output. Here’s an awesome diagram from &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;Jay Allamar’s blog post&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/intertoken-attn.png&quot; alt=&quot;drawing&quot; width=&quot;500&quot; /&gt;
    
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this case, the “it” token is getting &lt;i&gt;enriched by the “animal” token&lt;/i&gt;.&lt;/p&gt;

&lt;!-- Let's assume that there's only one attention head in each layer (to simplify). --&gt;

&lt;!-- &lt;div style=&quot;text-align: center&quot;&gt;
 &lt;img src=&quot;../assets/images/zoom-in-attention.png&quot; alt=&quot;drawing&quot; width=&quot;700&quot;/&gt;
&lt;/div&gt;

We'll assume: --&gt;

&lt;!-- &lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$h_i^{(l-1)} + m_i^{(l)} = x_i^{(l)}$$
&lt;/div&gt; --&gt;

&lt;!-- Now let's focus specifically on the attention head at layer $l$ and token position $i$. --&gt;

&lt;!-- &lt;div style=&quot;text-align: center; font-size: 60px&quot;&gt;
 $$v_0^{(l)}$$
 $$q_0^{(l)}$$
 $$k_0^{(l)}$$
&lt;/div&gt; --&gt;

&lt;!-- &lt;div style=&quot;text-align: center&quot;&gt;
 &lt;img src=&quot;../assets/images/attn_head.png&quot; alt=&quot;drawing&quot; width=&quot;500&quot;/&gt;
&lt;/div&gt;
&lt;br&gt; --&gt;

&lt;!-- But $a_i^{(l)}$ doesn't depend only on the token position $i$. It depends on the residual stream vector $x_i$ at all token positions from $i=0$ to $N$.

Think of it this way. Consider three linear maps that map the residual vector x to three different vectors, q, k, and v.

1. Query vector q
2. Key vector k
3. Value vector v

&lt;div style=&quot;text-align: center&quot;&gt;
 &lt;img src=&quot;../assets/images/qkv.png&quot; alt=&quot;drawing&quot; width=&quot;300&quot;/&gt;
&lt;/div&gt;
&lt;br&gt; --&gt;

&lt;!-- Now we'd have to apply the function to all token positions at layer l. --&gt;

&lt;!-- &lt;div style=&quot;text-align: center&quot;&gt;
 &lt;img src=&quot;../assets/images/qkv_all_pos.png&quot; alt=&quot;drawing&quot; width=&quot;300&quot;/&gt;
&lt;/div&gt;
&lt;br&gt; --&gt;

&lt;!-- &lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$a_i^{(l)} = (\sum_{j=0}^{N} A_{ij}^{(l)} v_{j}^{(l)})W_{out}$$
&lt;/div&gt; --&gt;

&lt;!-- So each token position somehow uses information from all the tokens to get the output of that block. In other words, our tokens are &quot;enriched&quot; by the other tokens in the input. Each $A_{ij}^{(l)}$ factor represents how much token $i$ is enriched by token $j$. This is called attention, a function of the queries and keys. --&gt;

&lt;!-- &lt;div style=&quot;text-align: center&quot;&gt;
 &lt;img src=&quot;../assets/images/Wout_attn.png&quot; alt=&quot;drawing&quot; width=&quot;500&quot;/&gt;
&lt;/div&gt; --&gt;

&lt;h4 id=&quot;attention-contributions&quot;&gt;Attention Contributions&lt;/h4&gt;

&lt;p&gt;We can directly probe the attention values during inference. While generating the output “Tamil”, we can check which tokens our final token is enriched by during this generation.&lt;/p&gt;

&lt;p&gt;The input to the LLM is a context plus a query. From now on, AT refers to the attribute token (the “Tamil” token within the context), and ST refers to the subject token (the “Chennai” token within the query).&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/AT_ST_C_Q.png&quot; alt=&quot;drawing&quot; width=&quot;1500&quot; /&gt;
    
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;They found out that the attention to the attribute token was much larger than the attention to the subject token (averaged across several examples). That shows that the LLM used the context to answer the question.&lt;/p&gt;

&lt;h4 id=&quot;attention-knockouts&quot;&gt;Attention Knockouts&lt;/h4&gt;

&lt;p&gt;We can also perform attention knockouts to confirm the importance of the attention values. This one is similar to causal analysis, where we knock out certain attention edges and see what effect that has on the final probability distribution and output.&lt;/p&gt;

&lt;p&gt;To knock out attention edge weights, we set the attention values to 0. Here, we ensure that the last token is &lt;strong&gt;not enriched&lt;/strong&gt; by the context’s &lt;strong&gt;attribute token&lt;/strong&gt; (“Tamil”).&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$A_{\text{AT,N}}^{(l)} = 0$$
&lt;/div&gt;

&lt;p&gt;This significantly changes the LLM’s output, causing it not to output “Tamil”.&lt;/p&gt;

&lt;p&gt;But let’s say we instead ensure the last token is not enriched by the query’s &lt;strong&gt;subject token&lt;/strong&gt; (“Chennai”).&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$A_{\text{ST,N}}^{(l)} = 0$$
&lt;/div&gt;

&lt;p&gt;The LLM may still output “Tamil”, showing that this time, the attention wasn’t really that important.&lt;/p&gt;

&lt;p&gt;Specifically, knocking out the attention edge to the subject token led to a 5% drop in the output probability (on average, across samples), while knocking out the attention edge to the attribute token led to a 20% drop in probability (on average).&lt;/p&gt;

&lt;div style=&quot;border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 20px&quot;&gt;
&lt;i&gt;
&lt;b&gt;Finding #2:&lt;/b&gt; Language models internally rely primarily on the context when it is provided to answer a question.&lt;/i&gt;
&lt;/div&gt;

&lt;h4 id=&quot;conclusions-and-limitations&quot;&gt;Conclusions and Limitations&lt;/h4&gt;

&lt;p&gt;The main takeaway of the paper is that during a RAG setup, when a context is provided, the LLM relies much less on parametric memory and more on context to answer the question. Even though this sounds trivial, it also reinstates the fact that the memory of the LLM is somehow encoded into the MLPs and that these MLPs become a lot less critical in RAG setups. Knowing more about the mechanisms of these processes can help us further optimise the processes in various ways and help us think of ways to change the behaviour of an LLM without ever having to retrain it. For example, if lawyers couldn’t use an LLM that isn’t trustworthy enough to &lt;i&gt;always&lt;/i&gt; output the correct answer as per the context, what if studying these attention weights and changing them in some way could force the LLM to stick to the context in a much more trustworthy fashion? Even if it’s not simple, we might get there soon.&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;

&lt;p&gt;The contents of this blog are from the paper &lt;a href=&quot;https://arxiv.org/abs/2406.12824&quot;&gt;From RAGs to rich parameters: Probing how language models utilise external knowledge over parametric information for factual queries&lt;/a&gt; by Hitesh Wadhwa et al. All diagrams were made using Canva.&lt;/p&gt;
</description>
        <pubDate>Sat, 28 Sep 2024 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/RAG/</link>
        <guid isPermaLink="true">http://localhost:4000/RAG/</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>Where are facts stored in Large Language Models?</title>
        <description>&lt;div style=&quot;border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 17px; text-align: center&quot;&gt;
    &lt;i&gt;
        &lt;a href=&quot;https://medium.com/@nikhilanandnj/where-are-facts-stored-in-large-language-models-0869914cfcbf&quot; target=&quot;_blank&quot;&gt;READ THE FULL ARTICLE ON MEDIUM&lt;/a&gt;
    &lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Brian Niccol became the CEO of Starbucks a few weeks ago. Unfortunately, our LLMs were trained much before this, so they might still think the CEO is Kevin Johnson. That begs the question, “How do we change facts in LLMs without retraining them every time?”, or perhaps an even deeper question, “Where are facts even stored in LLMs?”&lt;/p&gt;

&lt;p&gt;In 2022, Kevin Meng et al. wrote the paper &lt;a href=&quot;https://arxiv.org/abs/2202.05262&quot;&gt;Locating and Editing Factual Associations in GPT&lt;/a&gt;, detailing a method to identify where facts might be stored in LLMs and utilising this information to edit the facts in LLMs without any extra training. Note that training requires extensive GPU resources, so if we could edit facts just by changing a few weights, it would be faster and more resource-efficient.&lt;/p&gt;

&lt;p&gt;In this blog, we’ll go over how a method called causal mediation analysis is applied to LLMs to locate the fact storage sites in an LLM.&lt;/p&gt;

&lt;h3 id=&quot;a-recap-of-llm-architecture&quot;&gt;A Recap of LLM Architecture&lt;/h3&gt;

&lt;p&gt;If you don’t know how transformers work, I’d recommend going over &lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;Jay Allamar’s blog post&lt;/a&gt; “The Illustrated Transformer”, explaining how a transformer works in detail.&lt;/p&gt;

&lt;p&gt;Let’s assume $h_i^{(0)}$ is the hidden state before entering the transformer’s first layer. So, we can represent it as the sum of the embedded input vector and the positional embeddings of the input.&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$h_i^{(0)} = emb(x_i) + pos(i)$$
&lt;/div&gt;

&lt;p&gt;We add the attention layer output and the MLP layer output to the previous residual stream vector at every layer.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../assets/images/recurring_layer_equn.png&quot; alt=&quot;drawing&quot; width=&quot;430&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Here’s a simplified visualisation of a single layer in the transformer network. We can see that the previous layer’s hidden state for the token $i$ is denoted as $h_i^{(l-1)}$. This is inputted into the MLP layer (Multi-Layer Perceptron) to get $m_i^{(l)}$. This is added back into the hidden state (aka the residual stream), which is then passed into the attention block. The output from the attention block then goes back into the residual stream to get the hidden state of the current layer. This goes into the next layer, and the process repeats similarly.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/visual_layer2.png&quot; alt=&quot;drawing&quot; width=&quot;700&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We left out some details here, but that’s the gist.&lt;/p&gt;

&lt;h3 id=&quot;where-are-the-facts-stored-in-llms&quot;&gt;Where are the facts stored in LLMs?&lt;/h3&gt;

&lt;p&gt;Now we start by attempting to locate where in LLMs facts are stored through a method called &lt;strong&gt;Causal Mediation Analysis&lt;/strong&gt;. To implement this, we need to perform three runs: a clean run, a corrupted run, and a corrupted-with-restoration run.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/3runs2.png&quot; alt=&quot;drawing&quot; width=&quot;500&quot; /&gt;
&lt;/div&gt;

&lt;h4 id=&quot;1-clean-run&quot;&gt;1. Clean run&lt;/h4&gt;

&lt;p&gt;Let’s say we provide the string “Space Needle is located in the city of”, after which we expect GPT to output “Seattle”. In the clean run, no modifications were made to the LLM. The input is passed into the LLM, which predicts the next token. We’ll assume there are $T$ tokens in the input.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/cleanrun.png&quot; alt=&quot;drawing&quot; width=&quot;500&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The diagram shows that the input is passed into the embedder to get the embedded input. Then, the input is passed through several MLP and Attention layers. Finally, the LLM predicts the next token by unembedding the last hidden state. Since the run is clean, the output is not affected, and the factually correct output is expected (“Seattle”).&lt;/p&gt;

&lt;h4 id=&quot;2-corrupted-run&quot;&gt;2. Corrupted run&lt;/h4&gt;

&lt;p&gt;In the second run, we run the same input into the embedder, but we corrupt all tokens by adding some number $\epsilon$ to each token’s vector. We can see the summary in the below diagram. Naturally, corrupting the earlier layer causes all the later hidden states to be changed. We denote the corrupted hidden states by $h_i^{*(l)}$ for token $i$ and layer $l$.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/corrupted_run4.png&quot; alt=&quot;drawing&quot; width=&quot;550&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Due to the corruption of the embeddings before entering the first transformer layer, the output token is likely &lt;strong&gt;incorrect&lt;/strong&gt;. The output could be meaningless too, depending on the value of $\epsilon$ (how strong the perturbation is). For example, the above diagram shows a meaningless output, “cro”.&lt;/p&gt;

&lt;h4 id=&quot;3-corrupted-with-restoration-run&quot;&gt;3. Corrupted-with-restoration run&lt;/h4&gt;

&lt;p&gt;In the third run, we perform the corrupted run just like before, but we restore one specific token position at a particular layer (“corrupted with restoration”). Note that every other token in that layer remains corrupted.&lt;/p&gt;

&lt;p&gt;If this restoration, followed by a forward pass, leads to the correct answer, then the specific layer and token position we restored had some significance in determining the answer.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/corrupted_restoration3.png&quot; alt=&quot;drawing&quot; width=&quot;550&quot; /&gt;
&lt;/div&gt;

&lt;h4 id=&quot;total-effect&quot;&gt;Total Effect&lt;/h4&gt;

&lt;p&gt;Let’s consider our example: “Space Needle is located in the city of Seattle”.&lt;/p&gt;

&lt;p&gt;So, the expected output is “Seattle”.&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$o = \text {“Seattle”}$$
&lt;/div&gt;

&lt;p&gt;We define the Total Effect (TE) as:&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$\text{TE} = I\kern-0.3emP_*[o] - I\kern-0.3emP[o]$$
&lt;/div&gt;

&lt;p&gt;Here, $I\kern-0.3emP[o]$ represents the probability of outputting the “Seattle” token in the clean run, while $I\kern-0.3emP_*[o]$ represents the probability of outputting the “Seattle” token during the corrupted run. If it is high, we expect the LLM to output “Seattle”; if it is low, we expect the LLM to output a different token. For example, without corruption, “Seattle” may have a probability of 90%, but with corruption, its probability drops to 30%. Thus, we see that TE = 60%; therefore, corruption had a huge “Total Effect”.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/total_effect.png&quot; alt=&quot;drawing&quot; width=&quot;750&quot; /&gt;
&lt;/div&gt;

&lt;h4 id=&quot;indirect-effect&quot;&gt;Indirect Effect&lt;/h4&gt;

&lt;p&gt;We define the indirect effect of a specific layer $\hat{l}$ and token $\hat{i}$ as:&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$\text{IE} = I\kern-0.3emP_{*,\text{clean $h_i^{(l)}$}}[o] - I\kern-0.3emP_*[o]$$
&lt;/div&gt;

&lt;p&gt;Here, $I\kern-0.3emP_{*,\text{clean $h_i^{(l)}$}}[o]$ represents the final token probability for the “Seattle” token in the corrupted-with-restoration run, where $h_i^{(l)}$ is the restored hidden state. For example, if after corruption, the probability of “Seattle” is now 30%, but after restoring a specific layer and a specific token hidden state, the probability jumps to 50%, then that particular layer and token position has an indirect effect of 20%.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
    &lt;img src=&quot;../assets/images/indirect_effec.png&quot; alt=&quot;drawing&quot; width=&quot;750&quot; /&gt;
&lt;/div&gt;

&lt;h4 id=&quot;average-total-effect-ate-and-average-indirect-effect-aie&quot;&gt;Average Total Effect (ATE) and Average Indirect Effect (AIE)&lt;/h4&gt;

&lt;p&gt;Now, we want to see which locations in the model are responsible for remembering facts in general rather than specific facts. So, we average the total effect and indirect effects across several statements.&lt;/p&gt;

&lt;div style=&quot;border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 17px&quot;&gt;&lt;i&gt;
  &lt;li&gt;The larger the &lt;strong&gt;Average Total Effect&lt;/strong&gt; is, the more the corruption would've &lt;strong&gt;degraded&lt;/strong&gt; the output.&lt;/li&gt;

  &lt;li&gt;The larger the &lt;strong&gt;Average Indirect Effect&lt;/strong&gt; is, the more the restoration of a specific hidden state would've &lt;strong&gt;improved&lt;/strong&gt; the output, and the more &lt;strong&gt;important&lt;/strong&gt; that hidden state is.&lt;/li&gt;
&lt;/i&gt;&lt;/div&gt;

&lt;h4 id=&quot;so-what-were-the-results&quot;&gt;So what were the results?&lt;/h4&gt;

&lt;p&gt;The paper found that the Average Total Effect was 18.6% after corruption.&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px;&quot;&gt;
 $$\text{ATE} = 18.6\%$$
&lt;/div&gt;

&lt;p&gt;However, they also noticed that specific hidden states mediated much of this effect. For instance, the Average Indirect Effect of the &lt;strong&gt;last subject token&lt;/strong&gt; at &lt;strong&gt;layer 15&lt;/strong&gt; was 8.7%. The subject in our example is “Space Needle”, and the last token is “le”. So, the hidden state at this token, in layer 15, holds the largest share of the Total Effect across all hidden states.&lt;/p&gt;

&lt;div style=&quot;border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 17px&quot;&gt;&lt;i&gt;
  In English, the subject of the sentence refers to the person, place or thing that performs the action in the sentence, usually placed before the verb.&lt;/i&gt;
&lt;/div&gt;

&lt;h4 id=&quot;separating-contributions-of-mlp-and-attention-layers&quot;&gt;Separating contributions of MLP and Attention layers&lt;/h4&gt;

&lt;p&gt;They did further causal analysis to understand whether the MLP layers are more critical for factual storage or the attention layers. To do this, they perform a corrupted-with-restoration run where, after corruption of the initial hidden states, they restore only the MLP hidden state $m_\hat{i}^{(\hat{l})}$ (or only the attention hidden state $a_\hat{i}^{(\hat{l})}$). They use this to find the Indirect Effect of each MLP or Attention block.&lt;/p&gt;

&lt;div style=&quot;text-align: center; font-size: 30px&quot;&gt;
 $$(\text{AIE of MLP})_{max} = 6.6\%$$
 $$(\text{AIE of Attn})_{max} = 1.6\%$$
&lt;/div&gt;

&lt;p&gt;The maximum Average Indirect Effect at the last subject token was 6.6% for MLP layers and 1.6% for Attention layers. Note that here, we are sticking to the last subject token, but we are maximising the AIE value across layers. It is clear that &lt;strong&gt;MLP layers had a more significant causal effect than attention layers&lt;/strong&gt;, and they concluded from this that the MLP layers at the last subject token had something to do with factual storage. Later on, they figured out an approach to editing facts within LLMs by focusing on MLP layers at the last subject token, but we’ll cover that in a later blog!&lt;/p&gt;

&lt;h4 id=&quot;conclusion-and-limitations&quot;&gt;Conclusion and Limitations&lt;/h4&gt;

&lt;p&gt;This approach, called Causal Mediation Analysis, helps determine which parts of an LLM are in charge of which tasks. Once we know the crucial sites for specific tasks, we can target those sites for doing that task better! For example, factual associations depend more significantly on MLP layers at the last subject token. In fact, the MLPs probably act like databases, where you input a question and it retrieves the answer through a simple mapping.&lt;/p&gt;

&lt;p&gt;There are, however, some limitations. While MLP layers seem important for factual associations, we can’t infer that the other types of layers are not, and we still haven’t been able to prove the exact mechanism for storing facts.&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;

&lt;p&gt;The contents of this blog are from the paper &lt;a href=&quot;https://arxiv.org/abs/2202.05262&quot;&gt;Locating and Editing Factual Associations in GPT&lt;/a&gt; by Kevin Meng et al. All diagrams were made using Canva.&lt;/p&gt;
</description>
        <pubDate>Sat, 14 Sep 2024 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/rome-editing/</link>
        <guid isPermaLink="true">http://localhost:4000/rome-editing/</guid>
        
        
        <category>review</category>
        
      </item>
    
  </channel>
</rss>
