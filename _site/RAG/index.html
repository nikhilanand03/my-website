<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo2.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

<title>What makes RAG so effective? (How Attention Shapes Understanding in LLMs) | Nikhil Anand</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>What makes RAG so effective? (How Attention Shapes Understanding in LLMs) | Nikhil Anand</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="What makes RAG so effective? (How Attention Shapes Understanding in LLMs)" />
<meta name="author" content="nikhil" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/RAG/" />
<meta property="og:url" content="http://localhost:4000/RAG/" />
<meta property="og:site_name" content="Nikhil Anand" />
<meta property="og:image" content="http://localhost:4000/assets/images/attention_rag_blog.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-09-28T00:00:00+05:30" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/assets/images/attention_rag_blog.png" />
<meta property="twitter:title" content="What makes RAG so effective? (How Attention Shapes Understanding in LLMs)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"nikhil"},"dateModified":"2024-09-28T00:00:00+05:30","datePublished":"2024-09-28T00:00:00+05:30","headline":"What makes RAG so effective? (How Attention Shapes Understanding in LLMs)","image":"http://localhost:4000/assets/images/attention_rag_blog.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/RAG/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo2.png"},"name":"nikhil"},"url":"http://localhost:4000/RAG/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'],['\\(','\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>

<script src="mathjax-config.js" defer></script>

<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

</head>




<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
        <img src="/assets/images/logo2.png" alt="Nikhil Anand" style="width: 90px; height: auto;">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                    <a class="nav-link" href="/about">About</a>
                </li>

                
                <li class="nav-item">
                
                <a class="nav-link" href="/blogs">Blog</a>
                </li>

                <!--
                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>
                
                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/wowthemesnet/mediumish-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a>
                </li>
                -->

                <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->


<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">Nikhil Anand</h1>
    <p class="lead">
        
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=What makes RAG so effective? (How Attention Shapes Understanding in LLMs)&url=http://localhost:4000/RAG/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/RAG/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/RAG/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="/assets/images/pp_canada.jpeg" alt="Nikhil Anand">
                        
                    </div>
                    <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left">
                        <a target="_blank" class="link-dark" href="https://www.linkedin.com/in/nikhilanand1303/">Nikhil Anand</a><a target="_blank" href="https://twitter.com/nikhilanand003" class="btn follow">Follow</a>
                        <span class="author-description">Author of this site.</span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">What makes RAG so effective? (How Attention Shapes Understanding in LLMs)</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid" src="/assets/images/attention_rag_blog.png" alt="What makes RAG so effective? (How Attention Shapes Understanding in LLMs)">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                <!-- End Toc -->
                <p>Since ChatGPT came out, I remember seeing several blogs and videos talking about this new technique called Retrieval-Augmented Generation. It worked exceedingly well in preventing hallucinations, and ensuring factually correct outputs. Since then, startups and libraries have been built just to help people optimise this process for their use cases.</p>

<p>What is Retrieval-Augmented Generation? RAG is a technique that uses an ability of LLMs known as <strong>In-Context Learning</strong>. That means that if we provide a context with some information that an LLM doesn’t know, LLMs are adept at learning new information from the context and answering the question accordingly.</p>

<p>For example, the user asks the LLM <i>“What is the most popular language spoken in Chennai?”</i> and the LLM incorrectly outputs <i>“Hindi”</i>.</p>

<p><br /></p>
<div style="text-align: center;">
    <img src="../assets/images/chennai_hindi.png" alt="drawing" width="630" />
</div>
<p><br /></p>

<p>Now, the LLM might retrieve a Wikipedia article saying the following:</p>

<div style="border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 20px">
<i>
<b>Context: </b>
Tamil is the language spoken by most of Chennai's population; English is largely spoken by white-collar workers.[145][146] As per the 2011 census, Tamil is the most spoken language with 3,640,389 (78.3%) of speakers...
</i>
</div>

<p>Once the LLM has retrieved this from the web, it prepends this to the question and feeds it back into the LLM to get an answer.</p>

<p><br /></p>
<div style="text-align: center;">
    <img src="../assets/images/chennai_tamil.png" alt="drawing" width="630" />
</div>
<p><br /></p>

<p>Providing the context in the RAG setup leads to the correct answer, <i>“Tamil”</i>. One of the advantages of RAG is that it helps reduce hallucinations by ensuring the LLM first retrieves facts from the web and then at least gets the facts right.</p>

<p>But what’s going on internally?</p>

<h4 id="causal-tracing">Causal Tracing</h4>

<p>I wrote an <a href="https://medium.com/@nikhilanandnj/where-are-facts-stored-in-large-language-models-0869914cfcbf">earlier blog</a> that explains the process of Causal Tracing in detail. I’d greatly recommend going through that (specifically the section on “Causal Mediation Analysis”) before reading this blog.</p>

<p>Let’s consider the input: “What is the most popular language in <u>Chennai</u>?”</p>

<p>The idea behind causal tracing is figuring out which parts of the LLM are responsible for a particular behaviour. We saw over there that facts are stored in MLPs, and the MLP at the <strong>Last Subject Token (LST)</strong> significantly impacts the output. In this case, “<u>Chennai</u>” is the <strong>LST</strong>. We can test this through causal tracing by seeing whether ruining the input and restoring the MLP at the LST (Last Subject Token) will return the original input.</p>

<p>In the base case, we don’t perform corruption and the LLM outputs “Hindi”.</p>

<div style="text-align: center">
    <img src="../assets/images/mlp_causal1.png" alt="drawing" width="500" />
</div>
<p><br /></p>

<p>If we corrupt the LLM, we get the wrong output, but after restoring only the MLP at the LST, we see that the LLM brings back the original output.</p>

<div style="text-align: center">
    <img src="../assets/images/mlp_causal2.png" alt="drawing" width="1000" />
</div>
<p><br /></p>

<p>We average this value over several statements, so we know it happens for multiple statements, not just one.</p>

<p>However, let’s consider the RAG setup.</p>

<div style="text-align: center">
    <img src="../assets/images/mlp_notcausal1.png" alt="drawing" width="620" />
</div>
<p><br /></p>

<div style="text-align: center">
    <img src="../assets/images/mlp_notcausal2.png" alt="drawing" width="1000" />
</div>
<p><br /></p>

<p>The hypothesis stated in our <a href="https://medium.com/@nikhilanandnj/where-are-facts-stored-in-large-language-models-0869914cfcbf">previous blog</a> is that <strong>MLPs have something to do with factual recall</strong>. Restoring the MLP hidden states at the LST no longer returns the original output (“Tamil”). It looks like <strong>MLP activations aren’t that important anymore</strong>. Which means the LLM is <strong>no longer utilising its parametric memory</strong>. This makes sense since we expect it to focus on the context rather than use its parametric memory.</p>

<div style="border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 20px">
<i>
<b>Finding #1:</b> In the presence of a context, LLMs minimally use the parametric memory to answer a question.</i>
</div>

<p>So, if MLPs aren’t that important anymore, what is? <i>(Spoiler: Attention)</i></p>

<h4 id="how-attention-works-in-llms">How Attention Works in LLMs</h4>

<p>I’d recommend going over <a href="https://jalammar.github.io/illustrated-transformer/">Jay Allamar’s blog post</a> “The Illustrated Transformer” to better understand how attention works.</p>

<p>Attention is essential in LLMs because it allows LLMs to model the meaning of words based on other words in the input. For example, an “it” later in the sentence might refer to a noun that came before, and information needs to flow from the noun to the “it” to generate the next token. Thus, some dependencies must be facilitated across tokens to get the value of the attention block’s output. Here’s an awesome diagram from <a href="https://jalammar.github.io/illustrated-transformer/">Jay Allamar’s blog post</a>.</p>

<div style="text-align: center">
    <img src="../assets/images/intertoken-attn.png" alt="drawing" width="500" />
    
</div>
<p><br /></p>

<p>In this case, the “it” token is getting <i>enriched by the “animal” token</i>.</p>

<!-- Let's assume that there's only one attention head in each layer (to simplify). -->

<!-- <div style="text-align: center">
 <img src="../assets/images/zoom-in-attention.png" alt="drawing" width="700"/>
</div>

We'll assume: -->

<!-- <div style="text-align: center; font-size: 30px">
 $$h_i^{(l-1)} + m_i^{(l)} = x_i^{(l)}$$
</div> -->

<!-- Now let's focus specifically on the attention head at layer $l$ and token position $i$. -->

<!-- <div style="text-align: center; font-size: 60px">
 $$v_0^{(l)}$$
 $$q_0^{(l)}$$
 $$k_0^{(l)}$$
</div> -->

<!-- <div style="text-align: center">
 <img src="../assets/images/attn_head.png" alt="drawing" width="500"/>
</div>
<br> -->

<!-- But $a_i^{(l)}$ doesn't depend only on the token position $i$. It depends on the residual stream vector $x_i$ at all token positions from $i=0$ to $N$.

Think of it this way. Consider three linear maps that map the residual vector x to three different vectors, q, k, and v.

1. Query vector q
2. Key vector k
3. Value vector v

<div style="text-align: center">
 <img src="../assets/images/qkv.png" alt="drawing" width="300"/>
</div>
<br> -->

<!-- Now we'd have to apply the function to all token positions at layer l. -->

<!-- <div style="text-align: center">
 <img src="../assets/images/qkv_all_pos.png" alt="drawing" width="300"/>
</div>
<br> -->

<!-- <div style="text-align: center; font-size: 30px">
 $$a_i^{(l)} = (\sum_{j=0}^{N} A_{ij}^{(l)} v_{j}^{(l)})W_{out}$$
</div> -->

<!-- So each token position somehow uses information from all the tokens to get the output of that block. In other words, our tokens are "enriched" by the other tokens in the input. Each $A_{ij}^{(l)}$ factor represents how much token $i$ is enriched by token $j$. This is called attention, a function of the queries and keys. -->

<!-- <div style="text-align: center">
 <img src="../assets/images/Wout_attn.png" alt="drawing" width="500"/>
</div> -->

<h4 id="attention-contributions">Attention Contributions</h4>

<p>We can directly probe the attention values during inference. While generating the output “Tamil”, we can check which tokens our final token is enriched by during this generation.</p>

<div style="border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 18px">
<i>
The input to the LLM is a context plus a query. From now on, AT refers to the attribute token (the "Tamil" token within the context), and ST refers to the subject token (the "Chennai" token within the query).
</i>
</div>

<p>They found out that the attention to the attribute token was much larger than the attention to the subject token (averaged across several examples). That shows that the LLM used the context to answer the question.</p>

<h4 id="attention-knockouts">Attention Knockouts</h4>

<p>We can also perform attention knockouts to confirm the importance of the attention values. This one is similar to causal analysis, where we knock out certain attention edges and see what effect that has on the final probability distribution and output.</p>

<p>To knock out attention edge weights, we set the attention values to 0. Here, we ensure that the last token is <strong>not enriched</strong> by the context’s <strong>attribute token</strong> (“Tamil”).</p>

<div style="text-align: center; font-size: 30px">
 $$A_{\text{AT,N}}^{(l)} = 0$$
</div>

<p>This significantly changes the LLM’s output, causing it not to output “Tamil”.</p>

<p>But let’s say we instead ensure the last token is not enriched by the query’s <strong>subject token</strong> (“Chennai”).</p>

<div style="text-align: center; font-size: 30px">
 $$A_{\text{ST,N}}^{(l)} = 0$$
</div>

<p>The LLM may still output “Tamil”, showing that this time, the attention wasn’t really that important.</p>

<p>Specifically, knocking out the attention edge to the subject token led to a 5% drop in the output probability (on average, across samples), while knocking out the attention edge to the attribute token led to a 20% drop in probability (on average).</p>

<div style="border-left: 4px solid gray; background-color: #EAEAEA; padding: 10px; margin-bottom: 16px; font-size: 20px">
<i>
<b>Finding #2:</b> Language models internally rely primarily on the context when it is provided to answer a question.</i>
</div>

<h4 id="conclusions-and-limitations">Conclusions and Limitations</h4>

<p>The main takeaway of the paper is that during a RAG setup, when a context is provided, the LLM relies much less on parametric memory and more on context to answer the question. Even though this sounds trivial, it also reinstates the fact that the memory of the LLM is somehow encoded into the MLPs and that these MLPs become a lot less critical in RAG setups. Knowing more about the mechanisms of these processes can help us further optimise the processes in various ways and help us think of ways to change the behaviour of an LLM without ever having to retrain it. For example, if lawyers couldn’t use an LLM that isn’t trustworthy enough to <i>always</i> output the correct answer as per the context, what if studying these attention weights and changing them in some way could force the LLM to stick to the context in a much more trustworthy fashion? Even if it’s not simple, we might get there soon.</p>

<h4 id="acknowledgements">Acknowledgements</h4>

<p>The contents of this blog are from the paper <a href="https://arxiv.org/abs/2406.12824">From RAGs to rich parameters: Probing how language models utilise external knowledge over parametric information for factual queries</a> by Hitesh Wadhwa et al. All diagrams were made using Canva.</p>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2024-09-28">28 Sep 2024</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="/categories#review">review</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="//rome-editing/"> &laquo; Where are facts stored in Large Language Models?</a>
            
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'demowebsite'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<!-- <div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo2.png" alt="Nikhil Anand"> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://wowthemes.us11.list-manage.com/subscribe/post?u=8aeb20a530e124561927d3bd8&amp;id=8c3d2d214b" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>

    
</div> -->

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="/categories#review">review (2)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2024 Nikhil Anand 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>



<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//demowebsite.disqus.com/count.js"></script>


</body>
</html>
